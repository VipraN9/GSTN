# -*- coding: utf-8 -*-
"""GSTN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VJsen5oEWN457AAYBmYfq56E9vO91OJ3
"""

!pip install sklearn.preprocessing
!pip install shap
import pandas as pd
import numpy as np
import tensorflow as tf
import tensorflow.keras as keras
import matplotlib.pyplot as plt
from sklearn.cluster import  KMeans
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.metrics import f1_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV

# Load data
# Load your data
X_train = pd.read_csv('/content/sample_data/X_Train_Data_Input.csv', index_col=0)  # Replace with your file path
y_train = pd.read_csv('/content/sample_data/Y_Train_Data_Target.csv', index_col=0)  # Replace with your file path
X_test = pd.read_csv('/content/sample_data/X_Test_Data_Input.csv', index_col=0)  # Replace with your file path
y_test = pd.read_csv('/content/sample_data/Y_Test_Data_Target.csv', index_col=0)  # Replace with your file path
# Combine input and output data for cleaning
# data = pd.concat([input_data, output_data], axis=1)
# missing_values = input_data.isnull().sum()
# print(missing_values[missing_values > 0])
# X = input_data.drop(columns=['ID'])
# X.fillna(X.median(), inplace=True)
# Drop rows with NaN values
# X = input_data.fillna(median)
# y = output_data.fillna(median)
# # Separate cleaned input and output
# X.head()  # All columns except the last one
# Last column (target)
# X.isna().sum()

# X = pd.read_csv("/content/sample_data/X_Train_Data_Input.csv", index_col=0)
# X.info()

# y = pd.read_csv("/content/sample_data/Y_Train_Data_Target.csv", index_col=0)
# y.info()
# X_val = pd.read_csv("/content/sample_data/X_Test_Data_Input.csv", index_col=0)
# y_val = pd.read_csv("/content/sample_data/Y_Test_Data_Target.csv", index_col=0)
# Handle missing values (replace with mean or median)

# X_val.fillna(0)
# X_val.head()
# # Feature scaling (if necessary)
# scaler = StandardScaler()
# X = scaler.fit_transform(X)

# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train)
X_scaled

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# # Initialize and train the model
# rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
# rf_model.fit(X_train, y_train)

# # Make predictions
# y_pred = rf_model.predict(X_test)

# # Evaluate the model
# print("Accuracy:", accuracy_score(y_test, y_pred))
# print(classification_report(y_test, y_pred))

# model = Sequential()
# model.add(Dense(128, activation='relu', input_dim=X.shape[1]))  # Adjust input_dim based on features
# model.add(Dense(64, activation='relu'))
# model.add(Dense(1, activation='sigmoid'))  # Assuming binary classification

# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
# # Assuming you've preprocessed your data and created the model
# model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# # Make predictions on the validation set
# y_pred = model.predict(X_test)

# # Convert predicted probabilities to binary classes (assuming threshold of 0.5)
# y_pred_binary = (y_pred > 0.5).astype(int)

# # Evaluate accuracy and F1-score
# accuracy = accuracy_score(y_test, y_pred_binary)
# f1 = f1_score(y_test, y_pred_binary)

# print("Accuracy:", accuracy)
# print("F1-score:", f1)
# Build the Neural Network
nn_model = Sequential()
nn_model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))
nn_model.add(Dense(32, activation='relu'))
nn_model.add(Dense(1, activation='sigmoid'))  # Use 'softmax' for multi-class

# Using AdaGrad
# Using RMSprop
# Using SGD with momentum
# Using AdaGrad
nn_model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.001),
                 loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
nn_model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.35)

# Evaluate the model
loss, accuracy = nn_model.evaluate(X_test, y_test)
print("Accuracy:", accuracy)
print("Loss:", loss)

# model = Sequential()
# model.add(Dense(64, activation='relu', input_dim=22))  # Adjust input_dim based on features
# model.add(Dense(32, activation='relu'))
# model.add(Dense(1, activation='sigmoid'))
# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
# # Assuming you've preprocessed your data and created the model
# model.fit(X, y, epochs=10, batch_size=32, validation_data=(X_val, y_val))
# # Make predictions on the validation set
# y_pred = model.predict(X_val)

# # Convert predicted probabilities to binary classes (assuming threshold of 0.5)
# y_pred_binary = (y_pred > 0.5).astype(int)

# # Evaluate accuracy and F1-score
# accuracy = accuracy_score(y_val, y_pred_binary)
# f1 = f1_score(y_val, y_pred_binary)

# print("Accuracy:", accuracy)
# print("F1-score:", f1)
rf_model = RandomForestClassifier(n_estimators=200, min_samples_split=5, min_samples_leaf=2, max_depth=30, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions
y_pred = rf_model.predict(X_test)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

# Define the parameter grid
param_dist = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]  # Adding another parameter for more tuning
}

# Initialize RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=rf_model,
                                   param_distributions=param_dist,
                                   n_iter=20,  # Number of different combinations to try
                                   cv=3,
                                   scoring='accuracy',
                                   n_jobs=-1,  # Use all available cores
                                   random_state=42)

# Fit the model
random_search.fit(X_train, y_train)

# Output the best parameters and accuracy
print("Best parameters:", random_search.best_params_)
print("Best accuracy:", random_search.best_score_)

# log_reg = LogisticRegression()
# log_reg.fit(X_train, y_train)
# y_pred = log_reg.predict(X_test)
# print(accuracy_score(y_test, y_pred))
# print(classification_report(y_test, y_pred))

from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

# Assume X and y are your features and target variable
def assess_model_with_cross_validation(X, y):
    model = RandomForestClassifier(n_estimators=200, min_samples_split=5, min_samples_leaf=2, max_depth=30, random_state=42)
    scores = cross_val_score(model, X, y, cv=5)  # 5-fold cross-validation
    return scores


scores = assess_model_with_cross_validation(X_train, y_train)
print("Cross-Validation Scores:", scores, "Mean Score:", scores.mean())

from sklearn.metrics import classification_report, roc_auc_score

def evaluate_model_metrics(y_true, y_pred):
    print(classification_report(y_true, y_pred))
    roc_auc = roc_auc_score(y_true, y_pred)
    print(f"ROC AUC Score: {roc_auc}")

# Example usage after making predictions
rf_model.fit(X_train, y_train)
y_pred = rf_model.predict(X_test)
evaluate_model_metrics(y_test, y_pred)

def plot_feature_importance(X, y, feature_names):
    model = RandomForestClassifier(n_estimators=200, min_samples_split=10, min_samples_leaf=1, max_depth=20, random_state=42)
    model.fit(X, y)
    feature_importance = pd.Series(rf_model.feature_importances_, index=feature_names).sort_values(ascending=False)
    feature_importance.plot(kind='bar', figsize=(10, 6))
    plt.title('Feature Importance')
    plt.ylabel('Importance')
    plt.xlabel('Features')
    plt.show()

feature_names = X_train.columns.tolist()
plot_feature_importance(X_train, y_train, feature_names)

from matplotlib.colors import ListedColormap
# Select the significant features
X_significant_train = X_train[[X_train.columns[18], X_train.columns[1]]]  # Column 18 and Column 1
X_significant_test = X_test[[X_test.columns[18], X_test.columns[1]]]
# y = y_train.pop('target')
# Split the data for training and testing
rf_model = RandomForestClassifier(n_estimators=200, min_samples_split=5, min_samples_leaf=2, max_depth=30, random_state=42)
rf_model.fit(X_significant_train, y_train)



# Create a mesh grid for plotting decision boundaries
x_min, x_max = X_significant_train[X_train.columns[18]].min() - 1, X_significant_train[X_train.columns[18]].max() + 1
y_min, y_max = X_significant_train[X_train.columns[1]].min() - 1, X_significant_train[X_train.columns[1]].max() + 1

xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05),
                     np.arange(y_min, y_max, 0.05))

# Predict class probabilities for each point in the mesh grid
Z = rf_model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Plotting
plt.figure(figsize=(10, 6))
cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])
plt.contourf(xx, yy, Z, alpha=0.8, cmap=cmap_light)  # Decision boundary
sample_size = 1000
if len(X_significant_train) > sample_size:
    sampled_indices = np.random.choice(len(X_significant_train), sample_size, replace=False)
else:
    sampled_indices = np.arange(len(X_significant_train))

# plt.scatter(X_significant_train.iloc[sampled_indices][X_train.columns[18]],
#             X_significant_train.iloc[sampled_indices][X_train.columns[1]],
#             c=y.iloc[sampled_indices], edgecolors='k', marker='o', cmap=cmap_light)

plt.title('Decision Boundaries of Random Forest Model')
plt.xlabel('Feature from Column 18')
plt.ylabel('Feature from Column 1')
plt.colorbar(label='Class')
plt.show()

# Select the most important features based on importance scores
important_features = ['Column18', 'Column1', 'Column2', 'Column17', 'Column4', 'Column3', 'Column7', 'Column8']  # Replace with actual names
X_important_train = X_train[important_features]
X_important_test = X_test[important_features]


model_imp = RandomForestClassifier(n_estimators=200, min_samples_split=10,
                                    min_samples_leaf=1, max_depth=20, random_state=42)
model_imp.fit(X_important_train, y_train)

# Evaluate on the test set
y_pred_imp = model_imp.predict(X_important_test)
accuracy_imp = accuracy_score(y_test, y_pred_imp)

print(f"Accuracy with Important Features: {accuracy_imp}")

import shap


def explain_model_predictions(model, X):
  explainer = shap.TreeExplainer(model)
  sample_size = 10  # Adjust this number based on your dataset size
  X_sample = X.sample(n=sample_size, random_state=42)
  shap_values = explainer.shap_values(X_sample)  # Use the sampled data

    # Plot summary of SHAP values for important features
  # shap.summary_plot(shap_values[1], X)  # Assuming binary classification and focusing on class 1

# Example usage:
explain_model_predictions(rf_model, X_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
Ypred = rf_model.predict(X_test)

accuracy = accuracy_score(y_test, Ypred)
precision = precision_score(y_test, Ypred)
recall = recall_score(y_test, Ypred)
f1 = f1_score(y_test, Ypred)
roc_auc = roc_auc_score(y_test, rf_model.predict_proba(X_test)[:, 1])
conf_matrix = confusion_matrix(y_test, Ypred)

print(f"\n Performance:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"AUC-ROC: {roc_auc:.4f}")

import seaborn as sns

def plot_confusion_matrix(cm):
    # plt.figure(figsize=(8,6))
    # sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
    #             xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])

    # plt.ylabel('Actual')
    # plt.xlabel('Predicted')
    # plt.title('Confusion Matrix')
    # plt.show()
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    plt.figure(figsize=(8, 6))

    # Create a heatmap with annotations
    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap=plt.cm.Blues,
                xticklabels=class_names, yticklabels=class_names,
                cbar_kws={"orientation": "horizontal"}, square=True)

    plt.title('Confusion Matrix')
    plt.ylabel('Actual label')
    plt.xlabel('Predicted label')

    # Show the plot
    plt.tight_layout()
    plt.show()

# Plot confusion matrix for the best model (example: Random Forest)
plot_confusion_matrix(conf_matrix)

def plot_confusion_matrix(y_true, y_pred, class_names, title='Confusion Matrix', cmap=plt.cm.Blues):
    """
    Plots a confusion matrix using Seaborn's heatmap for better visualization.

    Parameters:
    - y_true: Actual target values
    - y_pred: Predicted target values
    - class_names: List of class names (labels)
    - title: Title for the plot
    - cmap: Colormap for the heatmap
    """

    # Calculate confusion matrix
    cm = confusion_matrix(y_true, y_pred)

    # Normalize the confusion matrix by row (i.e., by the number of samples in each class)
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    plt.figure(figsize=(8, 6))

    # Create a heatmap with annotations
    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap=cmap,
                xticklabels=class_names, yticklabels=class_names,
                cbar_kws={"orientation": "horizontal"}, square=True)

    plt.title(title)
    plt.ylabel('Actual label')
    plt.xlabel('Predicted label')

    # Show the plot
    plt.tight_layout()
    plt.show()

# Example usage:
# Assuming you have your true labels and predicted labels
# Replace these with your actual data
# y_test = np.array([0, 1, 0, 1, 0, 1])  # Example true labels
# y_pred = np.array([0, 0, 1, 1, 0, 1])  # Example predicted labels

# Class names for binary classification
class_names = ['Negative', 'Positive']

# Call the function to plot the confusion matrix
plot_confusion_matrix(y_test, Ypred, class_names)

from sklearn.metrics import log_loss, balanced_accuracy_score

# Assuming y_test contains the actual test labels

rf_model = RandomForestClassifier(n_estimators=200, min_samples_split=5, min_samples_leaf=2, max_depth=30, random_state=42)
rf_model.fit(X_train, y_train)

# Assuming best_rf_model is your trained Random Forest model
# Get predicted probabilities for the positive class
predicted_probas = rf_model.predict_proba(X_test)  # Shape: (n_samples, n_classes)

# Calculate Log Loss
log_loss_value = log_loss(y_test, predicted_probas)
print(f"Log Loss: {log_loss_value:.4f}")

# Get predicted classes
y_pred = rf_model.predict(X_test)

# Calculate Balanced Accuracy
balanced_accuracy_value = balanced_accuracy_score(y_test, y_pred)
print(f"Balanced Accuracy: {balanced_accuracy_value:.4f}")